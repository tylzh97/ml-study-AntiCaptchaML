#!/usr/bin/env python3
"""
æ¸è¿›å¼è®­ç»ƒè„šæœ¬ - åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šç”¨å›°éš¾æ•°æ®ç»§ç»­è®­ç»ƒ
"""
import os
import argparse
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import numpy as np
from pathlib import Path

from models import CRNN, CRNNLoss, BasicCRNN, ResNetCRNN
from utils import create_data_loaders
from utils.advanced_dataset import create_advanced_data_loaders
from train import Trainer


class ProgressiveTrainer(Trainer):
    """æ¸è¿›å¼è®­ç»ƒå™¨ - ç»§æ‰¿åŸºç¡€è®­ç»ƒå™¨"""
    
    def __init__(self, model, criterion, optimizer, scheduler, device, save_dir='./checkpoints'):
        super().__init__(model, criterion, optimizer, scheduler, device, save_dir)
        self.original_performance = {}
        self.hard_performance = {}
    
    def evaluate_on_both_datasets(self, easy_loader, hard_loader, dataset):
        """åœ¨ç®€å•å’Œå›°éš¾æ•°æ®é›†ä¸Šéƒ½è¿›è¡Œè¯„ä¼°"""
        print("ğŸ“Š è¯„ä¼°åŸå§‹æ•°æ®é›†æ€§èƒ½...")
        easy_loss, easy_char_acc, easy_seq_acc = self.validate(easy_loader, dataset)
        
        print("ğŸ”¥ è¯„ä¼°å›°éš¾æ•°æ®é›†æ€§èƒ½...")
        hard_loss, hard_char_acc, hard_seq_acc = self.validate(hard_loader, dataset)
        
        return {
            'easy': {'loss': easy_loss, 'char_acc': easy_char_acc, 'seq_acc': easy_seq_acc},
            'hard': {'loss': hard_loss, 'char_acc': hard_char_acc, 'seq_acc': hard_seq_acc}
        }
    
    def progressive_train(self, hard_train_loader, hard_val_loader, 
                         easy_val_loader, dataset, num_epochs, 
                         save_format='pth', warmup_epochs=5):
        """
        æ¸è¿›å¼è®­ç»ƒ
        
        Args:
            hard_train_loader: å›°éš¾æ•°æ®è®­ç»ƒåŠ è½½å™¨
            hard_val_loader: å›°éš¾æ•°æ®éªŒè¯åŠ è½½å™¨  
            easy_val_loader: åŸå§‹æ•°æ®éªŒè¯åŠ è½½å™¨
            dataset: æ•°æ®é›†å®ä¾‹
            num_epochs: è®­ç»ƒè½®æ•°
            save_format: ä¿å­˜æ ¼å¼
            warmup_epochs: çƒ­èº«è®­ç»ƒè½®æ•°
        """
        print(f"ğŸš€ å¼€å§‹æ¸è¿›å¼è®­ç»ƒï¼Œå…±{num_epochs}ä¸ªepoch")
        print(f"è®¾å¤‡: {self.device}")
        print(f"çƒ­èº«è®­ç»ƒ: {warmup_epochs} epochs")
        print(f"ä¿å­˜æ ¼å¼: {save_format}")
        
        best_easy_acc = 0.0
        best_hard_acc = 0.0
        best_overall_acc = 0.0
        
        # åˆå§‹è¯„ä¼°
        print("\nğŸ” è®­ç»ƒå‰æ€§èƒ½è¯„ä¼°:")
        initial_perf = self.evaluate_on_both_datasets(easy_val_loader, hard_val_loader, dataset)
        print(f"åŸå§‹æ•°æ®é›† - åºåˆ—å‡†ç¡®ç‡: {initial_perf['easy']['seq_acc']:.4f}")
        print(f"å›°éš¾æ•°æ®é›† - åºåˆ—å‡†ç¡®ç‡: {initial_perf['hard']['seq_acc']:.4f}")
        
        for epoch in range(1, num_epochs + 1):
            start_time = time.time()
            
            # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥
            if epoch <= warmup_epochs:
                # çƒ­èº«é˜¶æ®µï¼šä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = param_group['lr'] * 0.1
                print(f"ğŸ”¥ çƒ­èº«è®­ç»ƒé˜¶æ®µ ({epoch}/{warmup_epochs})")
            elif epoch == warmup_epochs + 1:
                # æ¢å¤æ­£å¸¸å­¦ä¹ ç‡
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = param_group['lr'] * 10
                print("âœ… çƒ­èº«å®Œæˆï¼Œæ¢å¤æ­£å¸¸å­¦ä¹ ç‡")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self.train_epoch(hard_train_loader, epoch)
            
            # åŒé‡éªŒè¯
            performance = self.evaluate_on_both_datasets(easy_val_loader, hard_val_loader, dataset)
            easy_seq_acc = performance['easy']['seq_acc']
            hard_seq_acc = performance['hard']['seq_acc']
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆåŸå§‹æ•°æ®æƒé‡æ›´é«˜ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ï¼‰
            overall_score = 0.7 * easy_seq_acc + 0.3 * hard_seq_acc
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if isinstance(self.scheduler, ReduceLROnPlateau):
                self.scheduler.step(overall_score)
            else:
                self.scheduler.step()
            
            epoch_time = time.time() - start_time
            
            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch}/{num_epochs}")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  åŸå§‹æ•°æ®é›† - æŸå¤±: {performance['easy']['loss']:.4f}, "
                  f"å­—ç¬¦å‡†ç¡®ç‡: {performance['easy']['char_acc']:.4f}, "
                  f"åºåˆ—å‡†ç¡®ç‡: {easy_seq_acc:.4f}")
            print(f"  å›°éš¾æ•°æ®é›† - æŸå¤±: {performance['hard']['loss']:.4f}, "
                  f"å­—ç¬¦å‡†ç¡®ç‡: {performance['hard']['char_acc']:.4f}, "
                  f"åºåˆ—å‡†ç¡®ç‡: {hard_seq_acc:.4f}")
            print(f"  ç»¼åˆåˆ†æ•°: {overall_score:.4f}")
            print(f"  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"  ç”¨æ—¶: {epoch_time:.1f}s")
            
            # æ€§èƒ½æå‡æ£€æŸ¥
            easy_improved = easy_seq_acc > best_easy_acc
            hard_improved = hard_seq_acc > best_hard_acc
            overall_improved = overall_score > best_overall_acc
            
            if easy_improved:
                best_easy_acc = easy_seq_acc
            if hard_improved:
                best_hard_acc = hard_seq_acc
            if overall_improved:
                best_overall_acc = overall_score
            
            # ä¿å­˜ç­–ç•¥
            save_best = False
            if overall_improved:
                save_best = True
                print(f"ğŸ‰ ç»¼åˆæ€§èƒ½æå‡! æ–°æœ€ä½³åˆ†æ•°: {overall_score:.4f}")
            elif easy_seq_acc >= initial_perf['easy']['seq_acc'] * 0.98 and hard_improved:
                # å¦‚æœåŸå§‹æ€§èƒ½æ²¡æœ‰æ˜¾è‘—ä¸‹é™ä¸”å›°éš¾æ•°æ®æ€§èƒ½æå‡
                save_best = True
                print(f"ğŸ”¥ å›°éš¾æ•°æ®æ€§èƒ½æå‡ä¸”åŸå§‹æ€§èƒ½ä¿æŒ!")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 5 == 0 or save_best:
                self.save_checkpoint(epoch, performance['hard']['loss'], overall_score, save_best, save_format)
            
            # æ—©åœæ£€æŸ¥
            if easy_seq_acc < initial_perf['easy']['seq_acc'] * 0.9:
                print(f"âš ï¸ è­¦å‘Š: åŸå§‹æ•°æ®æ€§èƒ½ä¸‹é™è¿‡å¤š ({easy_seq_acc:.4f} < {initial_perf['easy']['seq_acc']*0.9:.4f})")
                print("è€ƒè™‘é™ä½å­¦ä¹ ç‡æˆ–æå‰åœæ­¢è®­ç»ƒ")
            
            print("-" * 80)
        
        # æœ€ç»ˆæŠ¥å‘Š
        print(f"\nğŸ¯ æ¸è¿›å¼è®­ç»ƒå®Œæˆ!")
        print(f"åˆå§‹æ€§èƒ½ - åŸå§‹: {initial_perf['easy']['seq_acc']:.4f}, å›°éš¾: {initial_perf['hard']['seq_acc']:.4f}")
        print(f"æœ€ç»ˆæ€§èƒ½ - åŸå§‹: {best_easy_acc:.4f}, å›°éš¾: {best_hard_acc:.4f}")
        print(f"æœ€ä½³ç»¼åˆåˆ†æ•°: {best_overall_acc:.4f}")
        
        # æ€§èƒ½æå‡ç»Ÿè®¡
        easy_gain = best_easy_acc - initial_perf['easy']['seq_acc']
        hard_gain = best_hard_acc - initial_perf['hard']['seq_acc']
        print(f"æ€§èƒ½æå‡ - åŸå§‹: {easy_gain:+.4f}, å›°éš¾: {hard_gain:+.4f}")


def main():
    parser = argparse.ArgumentParser(description='æ¸è¿›å¼è®­ç»ƒCRNNéªŒè¯ç è¯†åˆ«æ¨¡å‹')
    
    # æ•°æ®ç›¸å…³
    parser.add_argument('--pretrained_model', type=str, required=True,
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--easy_data_root', type=str, default='./data',
                        help='åŸå§‹(ç®€å•)æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--hard_data_root', type=str, default='./hard_data',
                        help='å›°éš¾æ•°æ®æ ¹ç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', type=int, default=30,
                        help='æ¸è¿›è®­ç»ƒè½®æ•°')
    parser.add_argument('--warmup_epochs', type=int, default=5,
                        help='çƒ­èº«è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡(å»ºè®®æ¯”åˆå§‹è®­ç»ƒå°)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--backbone', type=str, default='resnet',
                        choices=['basic', 'resnet'], help='CNNéª¨å¹²ç½‘ç»œç±»å‹')
    parser.add_argument('--freeze_backbone', type=str, default='partial',
                        choices=['none', 'all', 'early', 'partial'],
                        help='ResNetéª¨å¹²ç½‘ç»œå†»ç»“ç­–ç•¥')
    parser.add_argument('--backbone_lr_ratio', type=float, default=0.05,
                        help='éª¨å¹²ç½‘ç»œå­¦ä¹ ç‡æ¯”ä¾‹(å»ºè®®æ›´å°)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./progressive_checkpoints',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_format', type=str, default='safetensors',
                        choices=['pth', 'safetensors'], help='ä¿å­˜æ ¼å¼')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--use_advanced_augment', action='store_true',
                        help='å¯¹å›°éš¾æ•°æ®ä½¿ç”¨é«˜çº§å¢å¼º')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(args.pretrained_model):
        print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {args.pretrained_model}")
        return
    
    if not os.path.exists(args.hard_data_root):
        print(f"âŒ å›°éš¾æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.hard_data_root}")
        print(f"è¯·å…ˆè¿è¡Œ: python generate_hard_data.py --output_dir {args.hard_data_root}")
        return
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½åŸå§‹æ•°æ®ï¼ˆç”¨äºå¯¹æ¯”è¯„ä¼°ï¼‰
    print("ğŸ“Š åŠ è½½åŸå§‹æ•°æ®é›†...")
    easy_train_loader, easy_val_loader, _, easy_dataset = create_data_loaders(
        args.easy_data_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    # åŠ è½½å›°éš¾æ•°æ®
    print("ğŸ”¥ åŠ è½½å›°éš¾æ•°æ®é›†...")
    if args.use_advanced_augment:
        hard_train_loader, hard_val_loader, _, _ = create_advanced_data_loaders(
            args.hard_data_root,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            augment_strength='light'  # å›°éš¾æ•°æ®ç”¨è½»åº¦å¢å¼ºé¿å…è¿‡åº¦
        )
    else:
        hard_train_loader, hard_val_loader, _, _ = create_data_loaders(
            args.hard_data_root,
            batch_size=args.batch_size,
            num_workers=args.num_workers
        )
    
    # åˆ›å»ºæ¨¡å‹
    if args.backbone == 'basic':
        model = BasicCRNN(60, 160, easy_dataset.num_classes)
    else:
        model = ResNetCRNN(60, 160, easy_dataset.num_classes, pretrained=False)
        # åº”ç”¨å†»ç»“ç­–ç•¥
        model.freeze_backbone(args.freeze_backbone)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
    if args.pretrained_model.endswith('.safetensors'):
        from safetensors.torch import load_file
        import json
        
        model_state_dict = load_file(args.pretrained_model)
        metadata_path = args.pretrained_model.replace('.safetensors', '_metadata.json')
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            print(f"é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯ - Epoch: {metadata.get('epoch', 'N/A')}, "
                  f"éªŒè¯å‡†ç¡®ç‡: {metadata.get('val_acc', 'N/A')}")
    else:
        checkpoint = torch.load(args.pretrained_model, map_location=device)
        model_state_dict = checkpoint['model_state_dict']
        print(f"é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯ - Epoch: {checkpoint.get('epoch', 'N/A')}, "
              f"éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('val_acc', 'N/A')}")
    
    model.load_state_dict(model_state_dict)
    model.to(device)
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    criterion = CRNNLoss()
    
    if args.backbone == 'resnet' and args.freeze_backbone != 'all':
        # å·®åˆ†å­¦ä¹ ç‡
        param_groups = model.get_param_groups(args.backbone_lr_ratio)
        optimizer_params = []
        for group in param_groups:
            optimizer_params.append({
                'params': group['params'],
                'lr': args.lr * group['lr_ratio']
            })
        optimizer = optim.Adam(optimizer_params, weight_decay=1e-5)  # æ›´å°çš„æƒé‡è¡°å‡
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)
    
    # ä½¿ç”¨ReduceLROnPlateauæ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)
    
    # åˆ›å»ºæ¸è¿›å¼è®­ç»ƒå™¨
    trainer = ProgressiveTrainer(model, criterion, optimizer, scheduler, device, args.save_dir)
    
    # å¼€å§‹æ¸è¿›å¼è®­ç»ƒ
    trainer.progressive_train(
        hard_train_loader, hard_val_loader, easy_val_loader,
        easy_dataset, args.num_epochs, args.save_format, args.warmup_epochs
    )


if __name__ == "__main__":
    import time
    main()