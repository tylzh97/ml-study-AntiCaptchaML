#!/usr/bin/env python3
import os
import time
import argparse
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
import numpy as np
from tqdm import tqdm

from models import CRNN, CRNNLoss, BasicCRNN, ResNetCRNN
from utils import create_data_loaders
from utils.advanced_dataset import create_advanced_data_loaders


class Trainer:
    def __init__(self, model, criterion, optimizer, scheduler, device, save_dir='./checkpoints'):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.save_dir = save_dir
        
        os.makedirs(save_dir, exist_ok=True)
        
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        
    def train_epoch(self, train_loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(train_loader)
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
        for batch_idx, (images, targets, target_lengths, texts) in enumerate(pbar):
            images = images.to(self.device)
            targets = targets.to(self.device)
            target_lengths = target_lengths.to(self.device)
            
            # å‰å‘ä¼ æ’­
            log_probs = self.model(images)
            seq_len, batch_size = log_probs.size(0), log_probs.size(1)
            input_lengths = torch.full((batch_size,), seq_len, dtype=torch.long).to(self.device)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(log_probs, targets, input_lengths, target_lengths)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/(batch_idx+1):.4f}'
            })
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        return avg_loss
    
    def validate(self, val_loader, dataset):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct_chars = 0
        total_chars = 0
        correct_sequences = 0
        total_sequences = 0
        
        with torch.no_grad():
            for images, targets, target_lengths, texts in tqdm(val_loader, desc='Validating'):
                images = images.to(self.device)
                targets = targets.to(self.device)
                target_lengths = target_lengths.to(self.device)
                
                # å‰å‘ä¼ æ’­
                log_probs = self.model(images)
                seq_len, batch_size = log_probs.size(0), log_probs.size(1)
                input_lengths = torch.full((batch_size,), seq_len, dtype=torch.long).to(self.device)
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(log_probs, targets, input_lengths, target_lengths)
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = torch.argmax(log_probs, dim=2)  # (seq_len, batch)
                predictions = predictions.permute(1, 0)  # (batch, seq_len)
                
                for i in range(batch_size):
                    pred_text = dataset.decode_prediction(predictions[i].cpu().numpy())
                    true_text = texts[i]
                    
                    # åºåˆ—å‡†ç¡®ç‡
                    if pred_text == true_text:
                        correct_sequences += 1
                    total_sequences += 1
                    
                    # å­—ç¬¦å‡†ç¡®ç‡
                    for pred_char, true_char in zip(pred_text, true_text):
                        if pred_char == true_char:
                            correct_chars += 1
                    total_chars += len(true_text)
        
        avg_loss = total_loss / len(val_loader)
        char_accuracy = correct_chars / total_chars if total_chars > 0 else 0
        seq_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0
        
        self.val_losses.append(avg_loss)
        self.val_accuracies.append(seq_accuracy)
        
        return avg_loss, char_accuracy, seq_accuracy
    
    def save_checkpoint(self, epoch, val_loss, val_acc, is_best=False, save_format='pth'):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'val_acc': val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
        }
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©æ–‡ä»¶æ‰©å±•å
        ext = 'safetensors' if save_format == 'safetensors' else 'pth'
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.save_dir, f'latest.{ext}')
        self._save_checkpoint_file(checkpoint, latest_path, save_format)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, f'best.{ext}')
            self._save_checkpoint_file(checkpoint, best_path, save_format)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹: val_acc={val_acc:.4f} (æ ¼å¼: {save_format})")
    
    def _save_checkpoint_file(self, checkpoint, filepath, save_format):
        """ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if save_format == 'safetensors':
            try:
                from safetensors.torch import save_file
                
                # SafeTensoråªèƒ½ä¿å­˜å¼ é‡ï¼Œéœ€è¦åˆ†åˆ«ä¿å­˜
                # 1. ä¿å­˜æ¨¡å‹æƒé‡ä¸ºsafetensors
                model_path = filepath
                save_file(checkpoint['model_state_dict'], model_path)
                
                # 2. ä¿å­˜å…¶ä»–å…ƒæ•°æ®ä¸ºjson
                import json
                metadata_path = filepath.replace('.safetensors', '_metadata.json')
                metadata = {
                    'epoch': checkpoint['epoch'],
                    'val_loss': checkpoint['val_loss'],
                    'val_acc': checkpoint['val_acc'],
                    'train_losses': checkpoint['train_losses'],
                    'val_losses': checkpoint['val_losses'],
                    'val_accuracies': checkpoint['val_accuracies'],
                }
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                # 3. ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ä¸ºpthï¼ˆsafetensorsä¸æ”¯æŒå¤æ‚å¯¹è±¡ï¼‰
                optimizer_path = filepath.replace('.safetensors', '_optimizer.pth')
                torch.save({
                    'optimizer_state_dict': checkpoint['optimizer_state_dict'],
                    'scheduler_state_dict': checkpoint['scheduler_state_dict'],
                }, optimizer_path)
                
                print(f"âœ“ SafeTensoræ ¼å¼ä¿å­˜: {model_path}")
                print(f"  - æ¨¡å‹æƒé‡: {model_path}")
                print(f"  - è®­ç»ƒå…ƒæ•°æ®: {metadata_path}")
                print(f"  - ä¼˜åŒ–å™¨çŠ¶æ€: {optimizer_path}")
                
            except ImportError:
                print("âš ï¸ safetensorsåº“æœªå®‰è£…ï¼Œå›é€€åˆ°PyTorchæ ¼å¼")
                torch.save(checkpoint, filepath.replace('.safetensors', '.pth'))
        else:
            # ä¼ ç»ŸPyTorchæ ¼å¼
            torch.save(checkpoint, filepath)
    
    def train(self, train_loader, val_loader, dataset, num_epochs, save_every=5, save_format='pth'):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        best_val_acc = 0.0
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œå…±{num_epochs}ä¸ªepoch")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        print(f"ä¿å­˜æ ¼å¼: {save_format}")
        
        for epoch in range(1, num_epochs + 1):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_loss, char_acc, seq_acc = self.validate(val_loader, dataset)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            epoch_time = time.time() - start_time
            
            # æ‰“å°ç»“æœ
            print(f"Epoch {epoch}/{num_epochs}")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  å­—ç¬¦å‡†ç¡®ç‡: {char_acc:.4f}")
            print(f"  åºåˆ—å‡†ç¡®ç‡: {seq_acc:.4f}")
            print(f"  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"  ç”¨æ—¶: {epoch_time:.1f}s")
            print("-" * 50)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = seq_acc > best_val_acc
            if is_best:
                best_val_acc = seq_acc
            
            if epoch % save_every == 0 or is_best:
                self.save_checkpoint(epoch, val_loss, seq_acc, is_best, save_format)
        
        print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒCRNNéªŒè¯ç è¯†åˆ«æ¨¡å‹')
    parser.add_argument('--data_root', type=str, default='./data', 
                        help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32, 
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=100, 
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-3, 
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--hidden_size', type=int, default=256, 
                        help='LSTMéšè—å±‚å¤§å°')
    parser.add_argument('--num_layers', type=int, default=2, 
                        help='LSTMå±‚æ•°')
    parser.add_argument('--img_height', type=int, default=60, 
                        help='å›¾åƒé«˜åº¦')
    parser.add_argument('--img_width', type=int, default=160, 
                        help='å›¾åƒå®½åº¦')
    parser.add_argument('--save_dir', type=str, default='./checkpoints', 
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--num_workers', type=int, default=4, 
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--resume', type=str, default='', 
                        help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--backbone', type=str, default='resnet', 
                        choices=['basic', 'resnet'], help='CNNéª¨å¹²ç½‘ç»œç±»å‹')
    parser.add_argument('--pretrained', action='store_true', default=True,
                        help='ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ (ä»…å¯¹resnetæœ‰æ•ˆ)')
    parser.add_argument('--freeze_backbone', type=str, default='early',
                        choices=['none', 'all', 'early', 'partial'],
                        help='ResNetéª¨å¹²ç½‘ç»œå†»ç»“ç­–ç•¥ (ä»…å¯¹resnetæœ‰æ•ˆ)')
    parser.add_argument('--backbone_lr_ratio', type=float, default=0.1,
                        help='éª¨å¹²ç½‘ç»œç›¸å¯¹å­¦ä¹ ç‡æ¯”ä¾‹ (ä»…å¯¹resnetæœ‰æ•ˆ)')
    parser.add_argument('--save_format', type=str, default='pth',
                        choices=['pth', 'safetensors'], 
                        help='æ¨¡å‹ä¿å­˜æ ¼å¼')
    parser.add_argument('--use_advanced_augment', action='store_true',
                        help='ä½¿ç”¨é«˜çº§æ•°æ®å¢å¼ºï¼ˆé€è§†å˜æ¢ã€å¼¹æ€§å½¢å˜ç­‰ï¼‰')
    parser.add_argument('--augment_strength', type=str, default='medium',
                        choices=['light', 'medium', 'heavy'],
                        help='æ•°æ®å¢å¼ºå¼ºåº¦')
    
    # å­¦ä¹ ç‡è°ƒåº¦å‚æ•°
    parser.add_argument('--step_size', type=int, default=10,
                        help='å­¦ä¹ ç‡è¡°å‡æ­¥é•¿ï¼ˆæ¯å¤šå°‘ä¸ªepochè¡°å‡ä¸€æ¬¡ï¼‰')
    parser.add_argument('--gamma', type=float, default=0.7,
                        help='å­¦ä¹ ç‡è¡°å‡å› å­')
    
    args = parser.parse_args()
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½å™¨
    if args.use_advanced_augment:
        print(f"ğŸš€ ä½¿ç”¨é«˜çº§æ•°æ®å¢å¼ºï¼Œå¼ºåº¦: {args.augment_strength}")
        train_loader, val_loader, test_loader, train_dataset = create_advanced_data_loaders(
            args.data_root,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            img_height=args.img_height,
            img_width=args.img_width,
            augment_strength=args.augment_strength
        )
    else:
        print("ğŸ“Š ä½¿ç”¨åŸºç¡€æ•°æ®å¢å¼º")
        train_loader, val_loader, test_loader, train_dataset = create_data_loaders(
            args.data_root,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            img_height=args.img_height,
            img_width=args.img_width
        )
    
    # æ¨¡å‹
    if args.backbone == 'basic':
        model = BasicCRNN(
            img_height=args.img_height,
            img_width=args.img_width,
            num_classes=train_dataset.num_classes,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers
        )
        print("ä½¿ç”¨åŸºç¡€CNNæ¶æ„")
    else:  # resnet
        model = ResNetCRNN(
            img_height=args.img_height,
            img_width=args.img_width,
            num_classes=train_dataset.num_classes,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            pretrained=args.pretrained
        )
        print(f"ä½¿ç”¨ResNet50æ¶æ„ï¼Œé¢„è®­ç»ƒæƒé‡: {args.pretrained}")
        
        # åº”ç”¨å†»ç»“ç­–ç•¥
        model.freeze_backbone(args.freeze_backbone)
    
    model.to(device)
    
    # æŸå¤±å‡½æ•°
    criterion = CRNNLoss()
    
    # ä¼˜åŒ–å™¨ - å¯¹ResNetä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    if args.backbone == 'resnet' and args.freeze_backbone != 'all':
        # è·å–å‚æ•°ç»„
        param_groups = model.get_param_groups(args.backbone_lr_ratio)
        optimizer_params = []
        for group in param_groups:
            optimizer_params.append({
                'params': group['params'],
                'lr': args.lr * group['lr_ratio']
            })
        optimizer = optim.Adam(optimizer_params, weight_decay=1e-4)
        print(f"ä½¿ç”¨å·®åˆ†å­¦ä¹ ç‡: ä¸»å¹²ç½‘ç»œlr={args.lr * args.backbone_lr_ratio:.6f}, å…¶ä»–éƒ¨åˆ†lr={args.lr:.6f}")
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
        print(f"ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡: lr={args.lr:.6f}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´é¢‘ç¹çš„è¡°å‡
    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)
    print(f"å­¦ä¹ ç‡è°ƒåº¦: æ¯{args.step_size}ä¸ªepochè¡°å‡è‡³{args.gamma}å€")
    
    # è®­ç»ƒå™¨
    trainer = Trainer(model, criterion, optimizer, scheduler, device, args.save_dir)
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 1
    if args.resume and os.path.exists(args.resume):
        print(f"ä» {args.resume} æ¢å¤è®­ç»ƒ")
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        trainer.train_losses = checkpoint.get('train_losses', [])
        trainer.val_losses = checkpoint.get('val_losses', [])
        trainer.val_accuracies = checkpoint.get('val_accuracies', [])
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader, train_dataset, args.num_epochs, save_format=args.save_format)


if __name__ == "__main__":
    main()